# data_preparation.py (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Language-Specific RAG)
# ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ ETL (Extract, Transform, Load)
# ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏™‡∏£‡πâ‡∏≤‡∏á Chunks ‡πÅ‡∏ö‡∏ö‡πÅ‡∏¢‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏° Metadata, ‡∏™‡∏£‡πâ‡∏≤‡∏á Embeddings, ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô FAISS Index

import os
import json
import time
import shutil
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv

# LangChain components
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.docstore.document import Document

# --- Configuration ---

# V V V --- ‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° --- V V V
USE_PROGRESS_LOG = False
# ^ ^ ^ --- ‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° --- ^ ^ ^

DATA_DIR = Path("data")
INDEX_DIR = Path("faiss_index")
DATA_FILE = DATA_DIR / "all_books.jsonl"
INDEX_PATH = INDEX_DIR / "book_index"
PROGRESS_FILE = DATA_DIR / "progress.log"

DATA_DIR.mkdir(exist_ok=True)
INDEX_DIR.mkdir(exist_ok=True)

# --- Helper Functions (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°) ---

def load_progress():
    if not USE_PROGRESS_LOG or not PROGRESS_FILE.exists():
        return set()
    with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f)

def save_progress(book_id):
    if not USE_PROGRESS_LOG:
        return
    with open(PROGRESS_FILE, "a", encoding="utf-8") as f:
        f.write(f"{book_id}\n")

# --- Main Script ---

def main():
    print("üöÄ Starting AI Librarian Data Preparation Script (Language-Specific RAG Version)...")
    print(f"üîÑ Progress Tracking Mode: {'ENABLED' if USE_PROGRESS_LOG else 'DISABLED'}")

    if not USE_PROGRESS_LOG:
        print("üî• Forcing a full rebuild...")
        if INDEX_PATH.exists():
            shutil.rmtree(INDEX_PATH)
            print("üóëÔ∏è  Deleted existing FAISS index.")
        if PROGRESS_FILE.exists():
            os.remove(PROGRESS_FILE)
            print("üóëÔ∏è  Deleted progress log file.")

    load_dotenv()
    google_api_key = os.getenv("GOOGLE_API_KEY_NNG") # <-- ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Key ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö .env ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    if not google_api_key:
        print("‚ùå Error: GOOGLE_API_KEY_NNG not found in .env file.")
        return

    if not DATA_FILE.exists():
        print(f"‚ùå Error: Data file not found at {DATA_FILE}")
        return

    print("‚ú® Initializing embedding model (text-embedding-004)...")
    try:
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=google_api_key,
            request_timeout=120
        )
    except Exception as e:
        print(f"‚ùå Error initializing embedding model: {e}")
        return

    processed_books = load_progress()
    if processed_books:
        print(f"‚úÖ Found {len(processed_books)} books already processed. They will be skipped.")

    all_docs = []
    new_book_count = 0
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()
        
    print("üìö Processing books from all_books.jsonl...")
    for line in tqdm(lines, desc="Processing Books"):
        try:
            book = json.loads(line)
            book_id = book.get("book_id")

            if not book_id or book_id in processed_books:
                continue

            ### <<< CHANGE START: NEW CHUNKING LOGIC >>> ###

            # --- 1. Process Thai Data ---
            title_th = book.get("title_th")
            if title_th:
                # 1.1 Create Thai Book Overview Chunk
                key_points_th_text = "\n".join(f"- {point}" for point in book.get("key_points_th", []))
                overview_content_th = (
                    f"‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠: {title_th}\n"
                    f"‡πÇ‡∏î‡∏¢: {', '.join(book.get('author_th', []))}\n"
                    f"‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ: {book.get('summary_th', '')}\n\n"
                    f"‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:\n{key_points_th_text}"
                )
                overview_metadata_th = {
                    "language": "th",
                    "book_id": book_id,
                    "source": title_th,
                    "chunk_type": "book_overview"
                }
                all_docs.append(Document(page_content=overview_content_th, metadata=overview_metadata_th))

                # 1.2 Create Thai Chapter Chunks
                for chapter in book.get("chapters_th", []):
                    chapter_content_th = (
                        f"‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠: {title_th}\n"
                        f"‡∏ö‡∏ó‡∏ó‡∏µ‡πà {chapter.get('chapter_number', 'N/A')}: {chapter.get('title', '')}\n\n"
                        f"‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {chapter.get('summary', '')}"
                    )
                    chapter_metadata_th = {
                        "language": "th",
                        "book_id": book_id,
                        "source": title_th,
                        "chunk_type": "chapter",
                        "chapter": chapter.get('chapter_number', 0)
                    }
                    all_docs.append(Document(page_content=chapter_content_th, metadata=chapter_metadata_th))

            # --- 2. Process English Data ---
            title_en = book.get("title_en")
            if title_en:
                # 2.1 Create English Book Overview Chunk
                key_points_en_text = "\n".join(f"- {point}" for point in book.get("key_points_en", []))
                overview_content_en = (
                    f"Book Overview: {title_en}\n"
                    f"By: {', '.join(book.get('author_en', []))}\n"
                    f"Summary: {book.get('summary_en', '')}\n\n"
                    f"Key Points:\n{key_points_en_text}"
                )
                overview_metadata_en = {
                    "language": "en",
                    "book_id": book_id,
                    "source": title_en,
                    "chunk_type": "book_overview"
                }
                all_docs.append(Document(page_content=overview_content_en, metadata=overview_metadata_en))

                # 2.2 Create English Chapter Chunks
                for chapter in book.get("chapters_en", []):
                    chapter_content_en = (
                        f"From the book: {title_en}\n"
                        f"Chapter {chapter.get('chapter_number', 'N/A')}: {chapter.get('title', '')}\n\n"
                        f"Content Summary: {chapter.get('summary', '')}"
                    )
                    chapter_metadata_en = {
                        "language": "en",
                        "book_id": book_id,
                        "source": title_en,
                        "chunk_type": "chapter",
                        "chapter": chapter.get('chapter_number', 0)
                    }
                    all_docs.append(Document(page_content=chapter_content_en, metadata=chapter_metadata_en))
            
            ### <<< CHANGE END >>> ###
            
            new_book_count += 1
            save_progress(book_id)

        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è Warning: Skipping a line due to invalid JSON format. Content: {line[:100]}")
            continue

    if not all_docs:
        print("\n‚úÖ No new books to process. The index is already up to date.")
        return

    print(f"\nCreated {len(all_docs)} new document chunks from {new_book_count} new book(s).")
    
    max_retries = 5
    retry_delay = 5

    for attempt in range(max_retries):
        try:
            if INDEX_PATH.exists() and USE_PROGRESS_LOG:
                print("üìÇ Loading existing FAISS index to add new data...")
                db = FAISS.load_local(str(INDEX_PATH), embeddings, allow_dangerous_deserialization=True)
                print(f"‚ûï Adding {len(all_docs)} new documents to the index...")
                db.add_documents(all_docs)
            else:
                print("‚ú® Creating new FAISS index from scratch...")
                db = FAISS.from_documents(all_docs, embeddings)
            
            print("üíæ Saving index to disk...")
            db.save_local(str(INDEX_PATH))
            print("üéâ Success! FAISS index has been created/updated successfully.")
            break

        except Exception as e:
            print(f"üî• An error occurred during embedding/indexing (Attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                print(f"Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                print("‚ùå Failed to create/update index after multiple retries.")
                break

if __name__ == "__main__":
    main()